{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_oai = OpenAI(api_key=openai_api_key)\n",
    "client_ds = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä½é¢è¯•å®˜ï¼Œè€Œæˆ‘å‡†å¤‡å‚åŠ å°å­¦æ•°å­¦æ•™å¸ˆæ‹›è˜é¢è¯•ï¼Œéœ€è¯•è®²äººæ•™ç‰ˆå°å­¦æ•°å­¦æŸèŠ‚é€‰ç‰‡æ®µï¼Œè¯•è®²åˆ†å¯¼å…¥ã€æ–°æˆã€ç»ƒä¹ å·©å›ºã€å°ç»“ã€ä½œä¸šäº”éƒ¨åˆ†ï¼ˆå¿…é¡»æœ‰è¿™äº”ä¸ªéƒ¨åˆ†ä¸”å¿…é¡»æŒ‰ç…§ä»¥ä¸‹é¡ºåºï¼‰ã€‚æ¥ä¸‹æ¥æˆ‘ä¼šä¸Šä¼ æˆ‘çš„è¯•è®²å†…å®¹ï¼Œè¯·ç”¨ä¸­æ–‡ç‚¹è¯„æˆ‘çš„è¯•è®²è¡¨ç°ï¼Œç»™å‡ºæˆ‘çš„ç¼ºç‚¹ï¼Œå¹¶å‘Šè¯‰æˆ‘å¦‚ä½•æ”¹æ­£ã€‚  \n",
    "1.å¯¼å…¥ï¼šç”ŸåŠ¨æœ‰è¶£ï¼Œå…·æ•™è‚²æ€§ï¼Œä½“ç°å­¦ç§‘èåˆä¸æ—¶æ”¿ï¼ˆå¦‚ï¼šâ€œåŒå­¦ä»¬ï¼Œå–œæ¬¢ã€Šè¥¿æ¸¸è®°ã€‹å—ï¼Ÿæœ€çˆ±è°ï¼Ÿå­™æ‚Ÿç©ºï¼Ÿå¯¹ï¼Œä»–ä¼šä¸ƒåäºŒå˜ï¼çœ‹å¤§å±å¹•ï¼Œé‡‘ç®æ£’æ—¶é•¿æ—¶çŸ­ï¼Œåƒæ–°é—»é‡Œçš„ä¼¸ç¼©ç§‘æŠ€ï¼Œæ€ä¹ˆé‡å®ƒï¼Ÿä»Šå¤©æˆ‘ä»¬å­¦ä¹ æ¯«ç±³ã€åˆ†ç±³ï¼â€ï¼‰ã€‚  \n",
    "2.æ–°æˆï¼šç›®æ ‡æ˜ç¡®ï¼Œé‡ç‚¹çªå‡ºï¼Œç¬¦åˆè¯¾æ ‡ä¸æ•™æï¼Œå†…å®¹å‡†ç¡®ï¼Œç»“æ„å®Œæ•´ï¼ŒæŒ–æ˜æ™ºèƒ½ä¸æ€æƒ³å› ç´ ï¼Œéš¾æ˜“é€‚åº¦ï¼Œæ­¥éª¤æ¸…æ™°ï¼Œé‡éš¾ç‚¹å¤„ç†å¾—å½“ï¼Œæ–¹æ³•çµæ´»ï¼Œå¯å‘æ€§å¼ºï¼Œä¸»å¯¼ä¸ä¸»ä½“ç»“åˆï¼Œæ€ç»´æ´»è·ƒï¼Œåé¦ˆè‰¯å¥½ã€‚  \n",
    "3.ç»ƒä¹ å·©å›ºï¼šå¸®åŠ©å­¦ç”Ÿå¯¹æ–°æˆçš„å†…å®¹è¿›è¡Œå·©å›ºç»ƒä¹ ï¼Œæ‰¿ä¸Šå¯ä¸‹ï¼Œæœ‰è¶£ï¼Œé¢å‘å…¨ä½“ï¼Œå…¨ç­è®¢æ­£ã€‚  \n",
    "4.å°ç»“ï¼šçŸ¥è¯†æ€§ä¸æ€æƒ³æ€§å…¼å¤‡ã€‚  \n",
    "5.æ³¨æ„ï¼šèå…¥å¤šæ ·æ•™å­¦æ³•ï¼ˆå¦‚å°ç»„è®¨è®ºã€å°æ¸¸æˆã€æƒ…å¢ƒå¦‚â€œä¹°ä¸œè¥¿æ‰¾é›¶â€ï¼‰ï¼Œç§¯æé¼“åŠ±ï¼Œäº’åŠ¨é¢‘ç¹ï¼Œæ—¶é•¿10-15åˆ†é’Ÿã€‚  \n",
    "æœ€åæä¾›æ¿ä¹¦è®¾è®¡ï¼ˆçªå‡ºé‡ç‚¹ï¼‰ã€‚\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(history, message):\n",
    "    for x in message[\"files\"]:\n",
    "        # æ£€æŸ¥æ˜¯å¦æ˜¯éŸ³é¢‘æ–‡ä»¶ï¼ˆå‡è®¾æ”¯æŒå¸¸è§æ ¼å¼å¦‚ .wav, .mp3ï¼‰\n",
    "        if x.lower().endswith((\".wav\", \".mp3\", \".m4a\")):\n",
    "            with open(x, \"rb\") as audio_file:\n",
    "                transcription = client_oai.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\", \n",
    "                    file=audio_file\n",
    "                )\n",
    "            history.append({\"role\": \"user\", \"content\": transcription.text})\n",
    "\n",
    "    if message[\"text\"] is not None:\n",
    "        history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
    "    return history, gr.MultimodalTextbox(value=None, interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oai_4omini_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_oai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oai_4o_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_oai.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_v3_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_ds.chat.completions.create(\n",
    "        model='deepseek-chat',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_r1_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_ds.chat.completions.create(\n",
    "        model='deepseek-reasoner',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot(history, model_choice):\n",
    "    # æ ¹æ® model_choice è°ƒç”¨ä¸åŒçš„ bot å‡½æ•°\n",
    "    if model_choice == \"GPT 4o-mini\":\n",
    "        yield from oai_4omini_bot(history)  # ä½¿ç”¨ yield from ä¼ é€’ç”Ÿæˆå™¨ç»“æœ\n",
    "    elif model_choice == \"GPT 4o\":\n",
    "        yield from oai_4o_bot(history)\n",
    "    elif model_choice == \"DeepSeek V3\":\n",
    "        yield from ds_v3_bot(history)\n",
    "    elif model_choice == \"DeepSeek R1\":\n",
    "        yield from ds_r1_bot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekin\\AppData\\Local\\Temp\\ipykernel_78368\\3652135178.py:5: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(elem_id=\"è¿™æ˜¯é™ˆè€å¸ˆåšçš„\", bubble_full_width=False, type=\"messages\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://7e7cc61ff4d0f610eb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7e7cc61ff4d0f610eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# Welcom Teacher Xie!ğŸ‘©ğŸ»â€ğŸ«\n",
    "                **è®©æˆ‘ä»¬ä¸€èµ·æ¥ç»ƒä¹ å§~ğŸ¤º**\n",
    "    \"\"\")\n",
    "    chatbot = gr.Chatbot(elem_id=\"è¿™æ˜¯é™ˆè€å¸ˆåšçš„\", bubble_full_width=False, type=\"messages\")\n",
    "\n",
    "    model_choice = gr.Dropdown(\n",
    "                label=\"é€‰æ‹©æ¨¡å‹\",\n",
    "                choices=[\"GPT 4o-mini\", \"GPT 4o\", \"DeepSeek V3\", \"DeepSeek R1\"],\n",
    "                value=\"GPT 4o-mini\"  # é»˜è®¤é€‰æ‹© \n",
    "            )\n",
    "\n",
    "    chat_input = gr.MultimodalTextbox(\n",
    "        interactive=True,\n",
    "        file_count=\"multiple\",\n",
    "        placeholder=\"Enter message or upload file...\",\n",
    "        show_label=False,\n",
    "        sources=[\"microphone\", \"upload\"],\n",
    "    )\n",
    "\n",
    "    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n",
    "    bot_msg = chat_msg.then(bot, [chatbot, model_choice], chatbot, api_name=\"bot_response\")\n",
    "    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
