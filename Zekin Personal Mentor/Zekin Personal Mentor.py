import os
import io
import requests 
from typing import List
from dotenv import load_dotenv
from openai import OpenAI
import gradio as gr
import base64
from PIL import Image

load_dotenv(override=True)
api_key = os.getenv('DEEPSEEK_API_KEY')
client_ds = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

gk_api_key = os.getenv('GROK_API_KEY')
client_gk = OpenAI(api_key=gk_api_key, base_url="https://api.x.ai/v1")

openai_api_key = os.getenv('OPENAI_API_KEY')
client_oai = OpenAI(api_key=openai_api_key)


system_prompt = """ä½ æ˜¯æˆ‘çš„ç§äººå­¦ä¹ æŒ‡å¯¼è€å¸ˆï¼Œå›ç­”åœ¨ä¿æŒç®€æ´çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä½“ç°ç»“æ„æ€§ã€‚
1. **çŸ¥è¯†ä¼ é€’åŸåˆ™**
   - ä¸‰çº§çŸ¥è¯†å¯†åº¦æ§åˆ¶ï¼š
     [1]æ ¸å¿ƒæ¦‚å¿µ â†’ [2]å…³é”®æ¨å¯¼ â†’ [3]æ‰©å±•è¾¹ç•Œ
   - è‡ªåŠ¨æ£€æµ‹çŸ¥è¯†ç›²åŒºæ—¶ï¼Œå¯åŠ¨ã€Œç»“æ„åŒ–è¡¥å…¨ã€ï¼š
     ``` 
     æ£€æµ‹åˆ°ä¸ç†Ÿæ‚‰RL â†’ ç”Ÿæˆå¼ºåŒ–å­¦ä¹ çŸ¥è¯†åœ°å›¾ï¼ˆåˆ†model-free/model-basedï¼‰
     ```
2. **æ•°å­¦å¤„ç†è§„èŒƒ**
   - æ¨å¯¼è¿‡ç¨‹å¿…é¡»åŒ…å«ï¼š
     [è¾“å…¥æ¡ä»¶] â†’ [å˜æ¢æ­¥éª¤] â†’ [ç»“è®ºéªŒè¯]
   - ç¤ºä¾‹ï¼š
     ```math
     \frac{d}{dx}e^x = \lim_{hâ†’0}\frac{e^{x+h}-e^x}{h} 
     = e^x \lim_{hâ†’0}\frac{e^h-1}{h} 
     = e^x \cdot 1 = e^x
     ```
3. **ç»“æ„åŒ–è¾“å‡ºå·¥å…·**
   - å¯¹æ¯”ç±»çŸ¥è¯†è‡ªåŠ¨è§¦å‘è¡¨æ ¼ï¼š
     | é‡åŒ–æ–¹æ³• | ç²¾åº¦æŸå¤± | ç¡¬ä»¶éœ€æ±‚ |
     |----------|----------|----------|
     | BNB      | ä½       | ä½       |
     | AWQ      | ä¸­       | é«˜       |
   - åˆ†ç±»çŸ¥è¯†ç”Ÿæˆæ€ç»´å¯¼å›¾ï¼š
     ```mermaid
     graph TD
     A[èšç±»æ–¹æ³•] --> B[åŸºäºè·ç¦»]
     A --> C[åŸºäºå¯†åº¦]
     ```
4. **äº¤äº’æ§åˆ¶æœºåˆ¶**
   - å½“æ‚¨è¯´"è¯¦ç»†æ¨å¯¼"æ—¶ï¼Œè‡ªåŠ¨å±•å¼€æ‰€æœ‰æ•°å­¦æ­¥éª¤
   - å½“æ‚¨è¯´"å¯¹æ¯”XXå’ŒXX"æ—¶ï¼Œå¼ºåˆ¶ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
   - æ£€æµ‹åˆ°å…³é”®è¯"æ€»ç»“"æ—¶ï¼Œè¾“å‡ºçŸ¥è¯†å¡ç‰‡ï¼š
     ```
     ã€çŸ¥è¯†å¡ç‰‡ã€‘Dropout
     - ä½œç”¨ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
     - æ•°å­¦å½¢å¼ï¼šBernoulliæ©ç 
     - å…¸å‹å€¼ï¼šp=0.5
     ```
ç‰¹åˆ«å…³æ³¨æˆ‘åœ¨æ¢ç´¢æ–°é¢†åŸŸã€æ–°çŸ¥è¯†æ—¶ï¼Œæ„Ÿè§‰æˆ‘å¯¹æŸæ–¹é¢çš„äº†è§£å¾ˆä¸å……è¶³æ—¶ï¼š
æ¯”å¦‚ï¼šæˆ‘è®©ä½ è®²è§£LLM Quantizationï¼Œæˆ‘é—®ä½ ä»€ä¹ˆæ˜¯bnbæ ¼å¼ï¼Œä½ å°±å¯ä»¥å‘Šè¯‰æˆ‘é™¤äº†bnbæ ¼å¼ï¼Œ
å¸¸ç”¨çš„è¿˜æœ‰awqæ ¼å¼ï¼Œawqé€‚åˆä½¿ç”¨vllmã€‚æˆ‘å¾ˆéœ€è¦è¿™æ ·ç»“æ„åŒ–çš„æ–¹å¼ç†è§£æ–°çš„é¢†åŸŸã€‚ä½†æ˜¯åˆ‡å¿Œç»™æˆ‘å¤ªå¤šçš„ç»†èŠ‚ï¼Œæ¯”å¦‚å‘Šè¯‰æˆ‘æ‰€æœ‰å­˜åœ¨çš„æ¨¡å‹æ ¼å¼ï¼Œ
å¹¶ä¸€ä¸€ä»‹ç»ä»–ä»¬ï¼Œä½ åªè¦å‘Šè¯‰æˆ‘ä¸»è¦çš„ï¼Œç‚¹åˆ°å³æ­¢å³å¯ï¼Œæˆ‘éœ€è¦è®²è§£æˆ‘ä¼šè¿›ä¸€æ­¥å‘Šè¯‰ä½ çš„ï¼
åœ¨è¾“å‡ºä»»ä½•æ•°å­¦å…¬å¼ã€ç¬¦å·æ—¶ï¼Œè¯·ä½¿ç”¨å…¬å¼æ ¼å¼ï¼ˆå…è®¸ä½¿ç”¨å—çº§å…¬å¼ï¼Œå¦‚ï¼š$$...$$ï¼‰è¾“å‡ºï¼Œæ ¸å¿ƒæ˜¯ç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°æ¸²æŸ“è¿‡çš„ï¼Œç¾è§‚æ˜“è¯»çš„å…¬å¼è¾“å‡ºï¼š
"""

def process_image(image_data):
    """ç»Ÿä¸€å¤„ç†æ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¯¹è±¡ã€Base64ç­‰å¤šç§è¾“å…¥"""
    try:
        # æƒ…å†µ1ï¼šæ”¶åˆ°çš„æ˜¯æ–‡ä»¶å¯¹è±¡ï¼ˆHugging Face Spacesï¼‰
        if hasattr(image_data, 'name'):  
            with open(image_data.name, "rb") as f:
                image_bytes = f.read()
            ext = image_data.name.split('.')[-1].lower()
        
        # æƒ…å†µ2ï¼šæ”¶åˆ°çš„æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
        elif isinstance(image_data, str) and os.path.exists(image_data):
            with open(image_data, "rb") as f:
                image_bytes = f.read()
            ext = image_data.split('.')[-1].lower()
        
        # æƒ…å†µ3ï¼šæ”¶åˆ°çš„æ˜¯PILå›¾åƒå¯¹è±¡
        elif hasattr(image_data, 'save'):
            buffered = io.BytesIO()
            image_data.save(buffered, format="PNG")
            image_bytes = buffered.getvalue()
            ext = "png"
        
        # ç»Ÿä¸€è½¬æ¢ä¸ºBase64
        image_base64 = base64.b64encode(image_bytes).decode('utf-8')
        return f"data:image/{ext};base64,{image_base64}"
    
    except Exception as e:
        print(f"Image processing error: {e}")
        return None

def add_message(history, message):
    image_extensions = (".png", ".jpg", ".jpeg", ".bmp", ".gif")
    
    if message["files"]:
        for file in message["files"]:
            # å¤„ç†å›¾ç‰‡æ–‡ä»¶
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_url = process_image(file)
                if not image_url:
                    continue
                    
                messages = [{
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": image_url, "detail": "high"}},
                        {"type": "text", "text": "æå–å›¾ç‰‡ä¸­çš„å†…å®¹å³å¯ï¼Œä¸è¦é¢å¤–æ·»åŠ ä»»ä½•æè¿°ã€‚æ•°å­¦å…¬å¼ç”¨LaTeXæ ¼å¼ï¼ˆå¦‚$$E=mc^2$$ï¼‰"}
                    ]
                }]
                
                try:
                    completion = client_gk.chat.completions.create(
                        model="grok-2-vision-latest",
                        messages=messages,
                        temperature=0.01
                    )
                    extracted_text = completion.choices[0].message.content
                    history.append({"role": "user", "content": f"å›¾ç‰‡å†…å®¹ï¼š\n{extracted_text}"})
                except Exception as e:
                    print(f"Vision API error: {e}")
                    history.append({"role": "user", "content": "å›¾ç‰‡è§£æå¤±è´¥"})
            
            # å¤„ç†æ–‡æœ¬æ–‡ä»¶
            elif file.lower().endswith('.txt'):
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        history.append({"role": "user", "content": f.read()})
                except:
                    history.append({"role": "user", "content": "æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥"})
    
    if message["text"]:
        history.append({"role": "user", "content": message["text"]})
    
    return history, gr.MultimodalTextbox(value=None, interactive=False)
    

def oai_4omini_bot(history: list):
    messages = [{"role": "system", "content": system_prompt}] + history
    stream = client_oai.chat.completions.create(
        model='gpt-4o-mini',
        messages=messages,
        stream=True)
    history.append({"role": "assistant", "content": ""})
    full_response = ''
    for chunk in stream:
        full_response += chunk.choices[0].delta.content or ''   
        history[-1]["content"] = full_response 
        yield history

def oai_4o_bot(history: list):
    messages = [{"role": "system", "content": system_prompt}] + history
    stream = client_oai.chat.completions.create(
        model='gpt-4o',
        messages=messages,
        stream=True)
    history.append({"role": "assistant", "content": ""})
    full_response = ''
    for chunk in stream:
        full_response += chunk.choices[0].delta.content or ''   
        history[-1]["content"] = full_response 
        yield history

def ds_v3_bot(history: list):
    messages = [{"role": "system", "content": system_prompt}] + history
    stream = client_ds.chat.completions.create(
        model='deepseek-chat',
        messages=messages,
        stream=True)
    history.append({"role": "assistant", "content": ""})
    full_response = ''
    for chunk in stream:
        full_response += chunk.choices[0].delta.content or ''   
        history[-1]["content"] = full_response 
        yield history

def ds_r1_bot(history: list):
    messages = [{"role": "system", "content": system_prompt}] + history
    stream = client_ds.chat.completions.create(
        model='deepseek-reasoner',
        messages=messages,
        stream=True)
    history.append({"role": "assistant", "content": ""})
    full_response = ''
    for chunk in stream:
        full_response += chunk.choices[0].delta.content or ''   
        history[-1]["content"] = full_response 
        yield history

def bot(history, model_choice):
    # æ ¹æ® model_choice è°ƒç”¨ä¸åŒçš„ bot å‡½æ•°
    if model_choice == "GPT 4o-mini":
        yield from oai_4omini_bot(history)  # ä½¿ç”¨ yield from ä¼ é€’ç”Ÿæˆå™¨ç»“æœ
    elif model_choice == "GPT 4o":
        yield from oai_4o_bot(history)
    elif model_choice == "DeepSeek V3":
        yield from ds_v3_bot(history)
    elif model_choice == "DeepSeek R1":
        yield from ds_r1_bot(history)

with gr.Blocks() as demo:
    gr.Markdown("""# Welcom Zekin!ğŸ‘¨ğŸ»â€ğŸš€
                **How Can I Assist With You~ğŸ¤º**
    """)
    chatbot = gr.Chatbot(elem_id="Zekin's Personal Mentor", 
                         bubble_full_width=False, 
                         type="messages",
                         height=550,
                         show_copy_button=True
                         )

    model_choice = gr.Dropdown(
                label="é€‰æ‹©æ¨¡å‹",
                choices=["DeepSeek V3", "DeepSeek R1", "GPT 4o-mini", "GPT 4o"],
                value="DeepSeek V3"  # é»˜è®¤é€‰æ‹© 
            )

    chat_input = gr.MultimodalTextbox(
        interactive=True,
        file_count="multiple",
        placeholder="è¾“å…¥æ–‡æœ¬æˆ–ç²˜è´´å›¾ç‰‡...",
        show_label=False,
        sources=["upload"]  # æ˜ç¡®æ”¯æŒå‰ªè´´æ¿
    )
    stop_btn = gr.Button("åœæ­¢")

    # å›è½¦æäº¤åŠŸèƒ½
    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])
    bot_msg = chat_msg.then(bot, [chatbot, model_choice], chatbot)
    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])

    stop_btn.click(None, cancels=[bot_msg])

demo.launch(share=True)