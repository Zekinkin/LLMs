{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekin\\AppData\\Local\\Temp\\ipykernel_42868\\1721435094.py:210: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(elem_id=\"Zekin's Personal Mentor\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7898/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7898/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://6b6700e5c10cc48231.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.x.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://6b6700e5c10cc48231.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6b6700e5c10cc48231.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import requests \n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "client_ds = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "gk_api_key = os.getenv('GROK_API_KEY')\n",
    "client_gk = OpenAI(api_key=gk_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client_oai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"你是我的私人学习指导老师，回答在保持简洁的同时，尽可能体现结构性。\n",
    "1. **知识传递原则**\n",
    "   - 三级知识密度控制：\n",
    "     [1]核心概念 → [2]关键推导 → [3]扩展边界\n",
    "   - 自动检测知识盲区时，启动「结构化补全」：\n",
    "     ``` \n",
    "     检测到不熟悉RL → 生成强化学习知识地图（分model-free/model-based）\n",
    "     ```\n",
    "2. **数学处理规范**\n",
    "   - 推导过程必须包含：\n",
    "     [输入条件] → [变换步骤] → [结论验证]\n",
    "   - 示例：\n",
    "     ```math\n",
    "     \\frac{d}{dx}e^x = \\lim_{h→0}\\frac{e^{x+h}-e^x}{h} \n",
    "     = e^x \\lim_{h→0}\\frac{e^h-1}{h} \n",
    "     = e^x \\cdot 1 = e^x\n",
    "     ```\n",
    "3. **结构化输出工具**\n",
    "   - 对比类知识自动触发表格：\n",
    "     | 量化方法 | 精度损失 | 硬件需求 |\n",
    "     |----------|----------|----------|\n",
    "     | BNB      | 低       | 低       |\n",
    "     | AWQ      | 中       | 高       |\n",
    "   - 分类知识生成思维导图：\n",
    "     ```mermaid\n",
    "     graph TD\n",
    "     A[聚类方法] --> B[基于距离]\n",
    "     A --> C[基于密度]\n",
    "     ```\n",
    "4. **交互控制机制**\n",
    "   - 当您说\"详细推导\"时，自动展开所有数学步骤\n",
    "   - 当您说\"对比XX和XX\"时，强制生成对比表格\n",
    "   - 检测到关键词\"总结\"时，输出知识卡片：\n",
    "     ```\n",
    "     【知识卡片】Dropout\n",
    "     - 作用：防止过拟合\n",
    "     - 数学形式：Bernoulli掩码\n",
    "     - 典型值：p=0.5\n",
    "     ```\n",
    "特别关注我在探索新领域、新知识时，感觉我对某方面的了解很不充足时：\n",
    "比如：我让你讲解LLM Quantization，我问你什么是bnb格式，你就可以告诉我除了bnb格式，\n",
    "常用的还有awq格式，awq适合使用vllm。我很需要这样结构化的方式理解新的领域。但是切忌给我太多的细节，比如告诉我所有存在的模型格式，\n",
    "并一一介绍他们，你只要告诉我主要的，点到即止即可，我需要讲解我会进一步告诉你的！\n",
    "在输出任何数学公式、符号时，请使用公式格式（允许使用块级公式，如：$$...$$）输出，核心是确保用户能看到渲染过的，美观易读的公式输出：\n",
    "\"\"\"\n",
    "\n",
    "def process_image(image_data):\n",
    "    \"\"\"统一处理文件路径、文件对象、Base64等多种输入\"\"\"\n",
    "    try:\n",
    "        # 情况1：收到的是文件对象（Hugging Face Spaces）\n",
    "        if hasattr(image_data, 'name'):  \n",
    "            with open(image_data.name, \"rb\") as f:\n",
    "                image_bytes = f.read()\n",
    "            ext = image_data.name.split('.')[-1].lower()\n",
    "        \n",
    "        # 情况2：收到的是本地文件路径\n",
    "        elif isinstance(image_data, str) and os.path.exists(image_data):\n",
    "            with open(image_data, \"rb\") as f:\n",
    "                image_bytes = f.read()\n",
    "            ext = image_data.split('.')[-1].lower()\n",
    "        \n",
    "        # 情况3：收到的是PIL图像对象\n",
    "        elif hasattr(image_data, 'save'):\n",
    "            buffered = io.BytesIO()\n",
    "            image_data.save(buffered, format=\"PNG\")\n",
    "            image_bytes = buffered.getvalue()\n",
    "            ext = \"png\"\n",
    "        \n",
    "        # 统一转换为Base64\n",
    "        image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "        return f\"data:image/{ext};base64,{image_base64}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Image processing error: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_message(history, message):\n",
    "    image_extensions = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")\n",
    "    \n",
    "    if message[\"files\"]:\n",
    "        for file in message[\"files\"]:\n",
    "            # 处理图片文件\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_url = process_image(file)\n",
    "                if not image_url:\n",
    "                    continue\n",
    "                    \n",
    "                messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url, \"detail\": \"high\"}},\n",
    "                        {\"type\": \"text\", \"text\": \"提取图片中的内容即可，不要额外添加任何描述。数学公式用LaTeX格式（如$$E=mc^2$$）\"}\n",
    "                    ]\n",
    "                }]\n",
    "                \n",
    "                try:\n",
    "                    completion = client_gk.chat.completions.create(\n",
    "                        model=\"grok-2-vision-latest\",\n",
    "                        messages=messages,\n",
    "                        temperature=0.01\n",
    "                    )\n",
    "                    extracted_text = completion.choices[0].message.content\n",
    "                    history.append({\"role\": \"user\", \"content\": f\"图片内容：\\n{extracted_text}\"})\n",
    "                except Exception as e:\n",
    "                    print(f\"Vision API error: {e}\")\n",
    "                    history.append({\"role\": \"user\", \"content\": \"图片解析失败\"})\n",
    "            \n",
    "            # 处理文本文件\n",
    "            elif file.lower().endswith('.txt'):\n",
    "                try:\n",
    "                    with open(file, 'r', encoding='utf-8') as f:\n",
    "                        history.append({\"role\": \"user\", \"content\": f.read()})\n",
    "                except:\n",
    "                    history.append({\"role\": \"user\", \"content\": \"文本文件读取失败\"})\n",
    "    \n",
    "    if message[\"text\"]:\n",
    "        history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
    "    \n",
    "    return history, gr.MultimodalTextbox(value=None, interactive=False)\n",
    "    \n",
    "\n",
    "def oai_4omini_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_oai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def oai_4o_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_oai.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def ds_v3_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_ds.chat.completions.create(\n",
    "        model='deepseek-chat',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def ds_r1_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_ds.chat.completions.create(\n",
    "        model='deepseek-reasoner',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def bot(history, model_choice):\n",
    "    # 根据 model_choice 调用不同的 bot 函数\n",
    "    if model_choice == \"GPT 4o-mini\":\n",
    "        yield from oai_4omini_bot(history)  # 使用 yield from 传递生成器结果\n",
    "    elif model_choice == \"GPT 4o\":\n",
    "        yield from oai_4o_bot(history)\n",
    "    elif model_choice == \"DeepSeek V3\":\n",
    "        yield from ds_v3_bot(history)\n",
    "    elif model_choice == \"DeepSeek R1\":\n",
    "        yield from ds_r1_bot(history)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# Welcom Zekin!👨🏻‍🚀\n",
    "                **How Can I Assist With You~🤺**\n",
    "    \"\"\")\n",
    "    chatbot = gr.Chatbot(elem_id=\"Zekin's Personal Mentor\", \n",
    "                         bubble_full_width=False, \n",
    "                         type=\"messages\",\n",
    "                         height=550,\n",
    "                         show_copy_button=True\n",
    "                         )\n",
    "\n",
    "    model_choice = gr.Dropdown(\n",
    "                label=\"选择模型\",\n",
    "                choices=[\"DeepSeek V3\", \"DeepSeek R1\", \"GPT 4o-mini\", \"GPT 4o\"],\n",
    "                value=\"DeepSeek V3\"  # 默认选择 \n",
    "            )\n",
    "\n",
    "    chat_input = gr.MultimodalTextbox(\n",
    "        interactive=True,\n",
    "        file_count=\"multiple\",\n",
    "        placeholder=\"输入文本或粘贴图片...\",\n",
    "        show_label=False,\n",
    "        sources=[\"upload\"]  # 明确支持剪贴板\n",
    "    )\n",
    "    stop_btn = gr.Button(\"停止\")\n",
    "\n",
    "    # 回车提交功能\n",
    "    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n",
    "    bot_msg = chat_msg.then(bot, [chatbot, model_choice], chatbot)\n",
    "    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
    "\n",
    "    stop_btn.click(None, cancels=[bot_msg])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
