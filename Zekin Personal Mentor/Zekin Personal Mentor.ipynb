{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekin\\AppData\\Local\\Temp\\ipykernel_42868\\1721435094.py:210: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(elem_id=\"Zekin's Personal Mentor\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7898/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7898/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://6b6700e5c10cc48231.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.x.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD https://6b6700e5c10cc48231.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6b6700e5c10cc48231.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import requests \n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "client_ds = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "gk_api_key = os.getenv('GROK_API_KEY')\n",
    "client_gk = OpenAI(api_key=gk_api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client_oai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"ä½ æ˜¯æˆ‘çš„ç§äººå­¦ä¹ æŒ‡å¯¼è€å¸ˆï¼Œå›ç­”åœ¨ä¿æŒç®€æ´çš„åŒæ—¶ï¼Œå°½å¯èƒ½ä½“ç°ç»“æ„æ€§ã€‚\n",
    "1. **çŸ¥è¯†ä¼ é€’åŸåˆ™**\n",
    "   - ä¸‰çº§çŸ¥è¯†å¯†åº¦æ§åˆ¶ï¼š\n",
    "     [1]æ ¸å¿ƒæ¦‚å¿µ â†’ [2]å…³é”®æ¨å¯¼ â†’ [3]æ‰©å±•è¾¹ç•Œ\n",
    "   - è‡ªåŠ¨æ£€æµ‹çŸ¥è¯†ç›²åŒºæ—¶ï¼Œå¯åŠ¨ã€Œç»“æ„åŒ–è¡¥å…¨ã€ï¼š\n",
    "     ``` \n",
    "     æ£€æµ‹åˆ°ä¸ç†Ÿæ‚‰RL â†’ ç”Ÿæˆå¼ºåŒ–å­¦ä¹ çŸ¥è¯†åœ°å›¾ï¼ˆåˆ†model-free/model-basedï¼‰\n",
    "     ```\n",
    "2. **æ•°å­¦å¤„ç†è§„èŒƒ**\n",
    "   - æ¨å¯¼è¿‡ç¨‹å¿…é¡»åŒ…å«ï¼š\n",
    "     [è¾“å…¥æ¡ä»¶] â†’ [å˜æ¢æ­¥éª¤] â†’ [ç»“è®ºéªŒè¯]\n",
    "   - ç¤ºä¾‹ï¼š\n",
    "     ```math\n",
    "     \\frac{d}{dx}e^x = \\lim_{hâ†’0}\\frac{e^{x+h}-e^x}{h} \n",
    "     = e^x \\lim_{hâ†’0}\\frac{e^h-1}{h} \n",
    "     = e^x \\cdot 1 = e^x\n",
    "     ```\n",
    "3. **ç»“æ„åŒ–è¾“å‡ºå·¥å…·**\n",
    "   - å¯¹æ¯”ç±»çŸ¥è¯†è‡ªåŠ¨è§¦å‘è¡¨æ ¼ï¼š\n",
    "     | é‡åŒ–æ–¹æ³• | ç²¾åº¦æŸå¤± | ç¡¬ä»¶éœ€æ±‚ |\n",
    "     |----------|----------|----------|\n",
    "     | BNB      | ä½       | ä½       |\n",
    "     | AWQ      | ä¸­       | é«˜       |\n",
    "   - åˆ†ç±»çŸ¥è¯†ç”Ÿæˆæ€ç»´å¯¼å›¾ï¼š\n",
    "     ```mermaid\n",
    "     graph TD\n",
    "     A[èšç±»æ–¹æ³•] --> B[åŸºäºè·ç¦»]\n",
    "     A --> C[åŸºäºå¯†åº¦]\n",
    "     ```\n",
    "4. **äº¤äº’æ§åˆ¶æœºåˆ¶**\n",
    "   - å½“æ‚¨è¯´\"è¯¦ç»†æ¨å¯¼\"æ—¶ï¼Œè‡ªåŠ¨å±•å¼€æ‰€æœ‰æ•°å­¦æ­¥éª¤\n",
    "   - å½“æ‚¨è¯´\"å¯¹æ¯”XXå’ŒXX\"æ—¶ï¼Œå¼ºåˆ¶ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼\n",
    "   - æ£€æµ‹åˆ°å…³é”®è¯\"æ€»ç»“\"æ—¶ï¼Œè¾“å‡ºçŸ¥è¯†å¡ç‰‡ï¼š\n",
    "     ```\n",
    "     ã€çŸ¥è¯†å¡ç‰‡ã€‘Dropout\n",
    "     - ä½œç”¨ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "     - æ•°å­¦å½¢å¼ï¼šBernoulliæ©ç \n",
    "     - å…¸å‹å€¼ï¼šp=0.5\n",
    "     ```\n",
    "ç‰¹åˆ«å…³æ³¨æˆ‘åœ¨æ¢ç´¢æ–°é¢†åŸŸã€æ–°çŸ¥è¯†æ—¶ï¼Œæ„Ÿè§‰æˆ‘å¯¹æŸæ–¹é¢çš„äº†è§£å¾ˆä¸å……è¶³æ—¶ï¼š\n",
    "æ¯”å¦‚ï¼šæˆ‘è®©ä½ è®²è§£LLM Quantizationï¼Œæˆ‘é—®ä½ ä»€ä¹ˆæ˜¯bnbæ ¼å¼ï¼Œä½ å°±å¯ä»¥å‘Šè¯‰æˆ‘é™¤äº†bnbæ ¼å¼ï¼Œ\n",
    "å¸¸ç”¨çš„è¿˜æœ‰awqæ ¼å¼ï¼Œawqé€‚åˆä½¿ç”¨vllmã€‚æˆ‘å¾ˆéœ€è¦è¿™æ ·ç»“æ„åŒ–çš„æ–¹å¼ç†è§£æ–°çš„é¢†åŸŸã€‚ä½†æ˜¯åˆ‡å¿Œç»™æˆ‘å¤ªå¤šçš„ç»†èŠ‚ï¼Œæ¯”å¦‚å‘Šè¯‰æˆ‘æ‰€æœ‰å­˜åœ¨çš„æ¨¡å‹æ ¼å¼ï¼Œ\n",
    "å¹¶ä¸€ä¸€ä»‹ç»ä»–ä»¬ï¼Œä½ åªè¦å‘Šè¯‰æˆ‘ä¸»è¦çš„ï¼Œç‚¹åˆ°å³æ­¢å³å¯ï¼Œæˆ‘éœ€è¦è®²è§£æˆ‘ä¼šè¿›ä¸€æ­¥å‘Šè¯‰ä½ çš„ï¼\n",
    "åœ¨è¾“å‡ºä»»ä½•æ•°å­¦å…¬å¼ã€ç¬¦å·æ—¶ï¼Œè¯·ä½¿ç”¨å…¬å¼æ ¼å¼ï¼ˆå…è®¸ä½¿ç”¨å—çº§å…¬å¼ï¼Œå¦‚ï¼š$$...$$ï¼‰è¾“å‡ºï¼Œæ ¸å¿ƒæ˜¯ç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°æ¸²æŸ“è¿‡çš„ï¼Œç¾è§‚æ˜“è¯»çš„å…¬å¼è¾“å‡ºï¼š\n",
    "\"\"\"\n",
    "\n",
    "def process_image(image_data):\n",
    "    \"\"\"ç»Ÿä¸€å¤„ç†æ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å¯¹è±¡ã€Base64ç­‰å¤šç§è¾“å…¥\"\"\"\n",
    "    try:\n",
    "        # æƒ…å†µ1ï¼šæ”¶åˆ°çš„æ˜¯æ–‡ä»¶å¯¹è±¡ï¼ˆHugging Face Spacesï¼‰\n",
    "        if hasattr(image_data, 'name'):  \n",
    "            with open(image_data.name, \"rb\") as f:\n",
    "                image_bytes = f.read()\n",
    "            ext = image_data.name.split('.')[-1].lower()\n",
    "        \n",
    "        # æƒ…å†µ2ï¼šæ”¶åˆ°çš„æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„\n",
    "        elif isinstance(image_data, str) and os.path.exists(image_data):\n",
    "            with open(image_data, \"rb\") as f:\n",
    "                image_bytes = f.read()\n",
    "            ext = image_data.split('.')[-1].lower()\n",
    "        \n",
    "        # æƒ…å†µ3ï¼šæ”¶åˆ°çš„æ˜¯PILå›¾åƒå¯¹è±¡\n",
    "        elif hasattr(image_data, 'save'):\n",
    "            buffered = io.BytesIO()\n",
    "            image_data.save(buffered, format=\"PNG\")\n",
    "            image_bytes = buffered.getvalue()\n",
    "            ext = \"png\"\n",
    "        \n",
    "        # ç»Ÿä¸€è½¬æ¢ä¸ºBase64\n",
    "        image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "        return f\"data:image/{ext};base64,{image_base64}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Image processing error: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_message(history, message):\n",
    "    image_extensions = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")\n",
    "    \n",
    "    if message[\"files\"]:\n",
    "        for file in message[\"files\"]:\n",
    "            # å¤„ç†å›¾ç‰‡æ–‡ä»¶\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_url = process_image(file)\n",
    "                if not image_url:\n",
    "                    continue\n",
    "                    \n",
    "                messages = [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url, \"detail\": \"high\"}},\n",
    "                        {\"type\": \"text\", \"text\": \"æå–å›¾ç‰‡ä¸­çš„å†…å®¹å³å¯ï¼Œä¸è¦é¢å¤–æ·»åŠ ä»»ä½•æè¿°ã€‚æ•°å­¦å…¬å¼ç”¨LaTeXæ ¼å¼ï¼ˆå¦‚$$E=mc^2$$ï¼‰\"}\n",
    "                    ]\n",
    "                }]\n",
    "                \n",
    "                try:\n",
    "                    completion = client_gk.chat.completions.create(\n",
    "                        model=\"grok-2-vision-latest\",\n",
    "                        messages=messages,\n",
    "                        temperature=0.01\n",
    "                    )\n",
    "                    extracted_text = completion.choices[0].message.content\n",
    "                    history.append({\"role\": \"user\", \"content\": f\"å›¾ç‰‡å†…å®¹ï¼š\\n{extracted_text}\"})\n",
    "                except Exception as e:\n",
    "                    print(f\"Vision API error: {e}\")\n",
    "                    history.append({\"role\": \"user\", \"content\": \"å›¾ç‰‡è§£æå¤±è´¥\"})\n",
    "            \n",
    "            # å¤„ç†æ–‡æœ¬æ–‡ä»¶\n",
    "            elif file.lower().endswith('.txt'):\n",
    "                try:\n",
    "                    with open(file, 'r', encoding='utf-8') as f:\n",
    "                        history.append({\"role\": \"user\", \"content\": f.read()})\n",
    "                except:\n",
    "                    history.append({\"role\": \"user\", \"content\": \"æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥\"})\n",
    "    \n",
    "    if message[\"text\"]:\n",
    "        history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
    "    \n",
    "    return history, gr.MultimodalTextbox(value=None, interactive=False)\n",
    "    \n",
    "\n",
    "def oai_4omini_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_oai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def oai_4o_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_oai.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def ds_v3_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_ds.chat.completions.create(\n",
    "        model='deepseek-chat',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def ds_r1_bot(history: list):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    stream = client_ds.chat.completions.create(\n",
    "        model='deepseek-reasoner',\n",
    "        messages=messages,\n",
    "        stream=True)\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    full_response = ''\n",
    "    for chunk in stream:\n",
    "        full_response += chunk.choices[0].delta.content or ''   \n",
    "        history[-1][\"content\"] = full_response \n",
    "        yield history\n",
    "\n",
    "def bot(history, model_choice):\n",
    "    # æ ¹æ® model_choice è°ƒç”¨ä¸åŒçš„ bot å‡½æ•°\n",
    "    if model_choice == \"GPT 4o-mini\":\n",
    "        yield from oai_4omini_bot(history)  # ä½¿ç”¨ yield from ä¼ é€’ç”Ÿæˆå™¨ç»“æœ\n",
    "    elif model_choice == \"GPT 4o\":\n",
    "        yield from oai_4o_bot(history)\n",
    "    elif model_choice == \"DeepSeek V3\":\n",
    "        yield from ds_v3_bot(history)\n",
    "    elif model_choice == \"DeepSeek R1\":\n",
    "        yield from ds_r1_bot(history)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# Welcom Zekin!ğŸ‘¨ğŸ»â€ğŸš€\n",
    "                **How Can I Assist With You~ğŸ¤º**\n",
    "    \"\"\")\n",
    "    chatbot = gr.Chatbot(elem_id=\"Zekin's Personal Mentor\", \n",
    "                         bubble_full_width=False, \n",
    "                         type=\"messages\",\n",
    "                         height=550,\n",
    "                         show_copy_button=True\n",
    "                         )\n",
    "\n",
    "    model_choice = gr.Dropdown(\n",
    "                label=\"é€‰æ‹©æ¨¡å‹\",\n",
    "                choices=[\"DeepSeek V3\", \"DeepSeek R1\", \"GPT 4o-mini\", \"GPT 4o\"],\n",
    "                value=\"DeepSeek V3\"  # é»˜è®¤é€‰æ‹© \n",
    "            )\n",
    "\n",
    "    chat_input = gr.MultimodalTextbox(\n",
    "        interactive=True,\n",
    "        file_count=\"multiple\",\n",
    "        placeholder=\"è¾“å…¥æ–‡æœ¬æˆ–ç²˜è´´å›¾ç‰‡...\",\n",
    "        show_label=False,\n",
    "        sources=[\"upload\"]  # æ˜ç¡®æ”¯æŒå‰ªè´´æ¿\n",
    "    )\n",
    "    stop_btn = gr.Button(\"åœæ­¢\")\n",
    "\n",
    "    # å›è½¦æäº¤åŠŸèƒ½\n",
    "    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n",
    "    bot_msg = chat_msg.then(bot, [chatbot, model_choice], chatbot)\n",
    "    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
    "\n",
    "    stop_btn.click(None, cancels=[bot_msg])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
