{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **------------------- Creat an IETLS Writting Instructor with RAG -------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "oai_key = os.getenv('OPENAI_API_KEY')\n",
    "ds_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "oai_client = OpenAI()\n",
    "ds_client = OpenAI(api_key=ds_key, base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **-------------------------------------------------------------------------------------------**\n",
    "# **First of all, we should Create a Chroma KnowledgeBase Base on Local Files**\n",
    "\n",
    "**Step 1 : Load Files**\n",
    "\n",
    "**Step 2 : Split Files into Chunks**\n",
    "\n",
    "**Step 3 : Vector Embedding -- Turn Chunks into Vectors(Using Auto-Encoding LLMS -- OpenAIEmbedding or BERT**\n",
    "\n",
    "**Step 4 : Pass Vectors into DataBase (eg.Chroma,FAISS)**\n",
    "\n",
    "# **-------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1 :  Load Files**\n",
    "\n",
    "**é€’å½’éå†æ–‡ä»¶å¤¹**ï¼šéœ€è¦å¤„ç†å¤šå±‚åµŒå¥—æ–‡ä»¶å¤¹ï¼Œä½¿ç”¨é€’å½’æ–¹æ³•è·å–æ‰€æœ‰æ–‡ä»¶è·¯å¾„ã€‚\n",
    "\n",
    "**æå–æ–‡ä»¶å¤¹åç§°ä½œä¸ºdoc_type**ï¼šå¯¹äºæ¯ä¸ªæ–‡ä»¶ï¼Œè®°å½•å®ƒæ‰€å±çš„ç›´æ¥çˆ¶æ–‡ä»¶å¤¹åç§°ã€‚\n",
    "\n",
    "**æ”¯æŒå¤šç§æ–‡ä»¶ç±»å‹**ï¼šæ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åˆé€‚çš„åŠ è½½å™¨ï¼ˆå¦‚PDFã€Wordã€Excelï¼‰ã€‚\n",
    "\n",
    "**å…ƒæ•°æ®æ·»åŠ **ï¼šå°†çˆ¶æ–‡ä»¶å¤¹åæ·»åŠ åˆ°æ¯ä¸ªæ–‡ä»¶çš„metadataä¸­ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`os`**ï¼šå®šä½çˆ¶æ–‡ä»¶å¤¹\n",
    "2. **`glob`**ï¼šéå†ç›®å½•è·å–æ–‡ä»¶åˆ—è¡¨\n",
    "3. **`pathlib`**ï¼šåˆ†ææ–‡ä»¶å±æ€§ï¼ˆå¦‚pdfã€wordã€markdownï¼‰\n",
    "4. **`LangChain`**ï¼šåŠ è½½æ–‡ä»¶å†…å®¹å¹¶è½¬ä¸ºDocumentå¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–‡ä»¶å¤¹è·¯å¾„\n",
    "folder = (r\"C:\\Users\\zekin\\projects\\llm_engineering\\My Projects.ipynb\\Week 5 Projects\\IELTS Writting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ”¯æŒçš„æ–‡ä»¶ç±»å‹åŠå¯¹åº”åŠ è½½å™¨\n",
    "file_loaders = {\n",
    "    \".pdf\": PyPDFLoader,       # PDFå¤„ç†å‡½æ•°\n",
    "    \".md\": TextLoader,         # Markdownå¤„ç†å‡½æ•°\n",
    "    \".docx\": Docx2txtLoader,   # Wordå¤„ç†å‡½æ•°\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­˜å‚¨æ‰€æœ‰æ–‡æ¡£\n",
    "documents = []\n",
    "\n",
    "# é€’å½’è·å–æ‰€æœ‰æ–‡ä»¶å¹¶åŠ è½½\n",
    "def process_folder(folder_path):\n",
    "    for item in glob(os.path.join(folder_path, \"*\")):\n",
    "        if os.path.isdir(item):\n",
    "            # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œé€’å½’å¤„ç†\n",
    "            process_folder(item)\n",
    "        else:\n",
    "            # å¦‚æœæ˜¯æ–‡ä»¶ï¼ŒåŠ è½½å¹¶æ·»åŠ å…ƒæ•°æ®\n",
    "            file_ext = Path(item).suffix.lower()\n",
    "            if file_ext in file_loaders:\n",
    "                loader = file_loaders[file_ext](item)  # è®²itemä¼ å…¥å¯¹åº”çš„åŠ è½½å™¨\n",
    "                docs = loader.load()   # åŠ è½½å¯¹åº”çš„æ–‡ä»¶\n",
    "                # è·å–ç›´æ¥çˆ¶æ–‡ä»¶å¤¹åç§°\n",
    "                doc_type = os.path.basename(os.path.dirname(item))\n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"doc_type\"] = doc_type\n",
    "                    documents.append(doc)\n",
    "            elif file_ext == \".xlsx\":\n",
    "                excel_file = pd.read_excel(item, engine=\"openpyxl\")\n",
    "                content = excel_file.to_string()\n",
    "                doc = Document(page_content=content,metadata={'source':item})\n",
    "                doc_type = os.path.basename(os.path.dirname(item))\n",
    "                doc.metadata['doc_type'] = doc_type\n",
    "                documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2 :  Split Text Into Chunk**\n",
    "\n",
    "**ç”¨ langchain çš„ CharacterTextSplitter å°†æ–‡æ¡£åˆ†å‰²æˆå°å—ï¼ˆchunksï¼‰ï¼Œä¾¿äºå¤„ç†**\n",
    "\n",
    "```chunk_size```: 1000 å­—ç¬¦ â‰ˆ 200-300 token  \n",
    "æ£€ç´¢ä»»åŠ¡ï¼šè¾ƒå°çš„å—ï¼ˆ500-1500ï¼‰ä¾¿äºç²¾ç¡®åŒ¹é…   \n",
    "æ€»ç»“ä»»åŠ¡ï¼šè¾ƒå¤§å—ï¼ˆ1000-2000ï¼‰ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡\n",
    "\n",
    "```vchunk_overlap```: ç›¸é‚»å—ä¹‹é—´é‡å¤çš„å­—ç¬¦æ•°,é‡å é¿å…å…³é”®ä¿¡æ¯åœ¨å—è¾¹ç•Œä¸¢å¤±  \n",
    "å¯†é›†ä¿¡æ¯ï¼ˆå¦‚æŠ€æœ¯æ–‡æ¡£ï¼‰ï¼šå¢å¤§é‡å ï¼ˆ200-300ï¼‰   \n",
    "ç¨€ç–æ–‡æœ¬ï¼ˆå¦‚å¯¹è¯ï¼‰ï¼šå‡å°é‡å ï¼ˆ50-100ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  # å°†å¤§æ–‡æ¡£æŒ‰å­—ç¬¦æ•°åˆ‡åˆ†ä¸ºå°å—ï¼Œè®¾ç½®é‡å ä»¥ä¿ç•™ä¸Šä¸‹æ–‡\n",
    "chunks = text_splitter.split_documents(documents)  # å°†æ¯ä¸ªæ–‡æ¡£åˆ†å‰²æˆå°å—ï¼Œè¿”å›æ–°çš„æ–‡æ¡£å—åˆ—è¡¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3 :  Vecot Embedding -- Turn Chunks into Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use **OpenAIEmbeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create our Chroma vectorstore!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 178 documents\n"
     ]
    }
   ],
   "source": [
    "db_name = \"IEKTS_vector_db\"\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get one vector and find how many dimensions it has**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection  # å¾—åˆ°çš„æ˜¯ Chroma å†…éƒ¨çš„ é›†åˆå¯¹è±¡\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Vectordtore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"vectorstore\")\n",
    "vectorstore.persist()  # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ \"vectorstore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4 :  A Conversation Chain with RAG and Memory + A Gradio Interface**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Method 1 : Use A Pipeline  -- ConversationalRetrievalChain.from_llm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¯´æ˜**\n",
    "- `ConversationBufferMemory` ç”¨äºä¿å­˜å¯¹è¯å†å²ï¼Œå¸¸ä¸ `ConversationalRetrievalChain` é…åˆã€‚\n",
    "- `memory_key` å’Œ `return_messages` åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®ï¼Œå½±å“åç»­æ•°æ®è®¿é—®å’Œæ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekin\\AppData\\Local\\Temp\\ipykernel_39264\\2907641848.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\zekin\\AppData\\Local\\Temp\\ipykernel_39264\\2907641848.py:15: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True) # è¿”å›å€¼ï¼šä¸€ä¸ªé…ç½®å¥½çš„å†…å­˜å®ä¾‹ï¼Œç”¨äºè·Ÿè¸ªå¯¹è¯ã€‚\n"
     ]
    }
   ],
   "source": [
    "# LangChain ä¸­åˆå§‹åŒ–ä¸€ä¸ª DeepSeek èŠå¤©æ¨¡å‹\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name=\"deepseek-chat\",                  \n",
    "    openai_api_key=ds_key,                         # DeepSeek çš„ API å¯†é’¥\n",
    "    openai_api_base=\"https://api.deepseek.com/v1\",  # DeepSeek API ç«¯ç‚¹\n",
    "    streaming=True  # å¯ç”¨æµå¼è¾“å‡º\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªå¯¹è¯ç¼“å†²å†…å­˜\n",
    "# chat_historyï¼šåç»­ä»£ç ä¼šé€šè¿‡è¿™ä¸ªé”®è®¿é—®å†å²\n",
    "# return_messages=True: è¿”å›ç»“æ„åŒ–çš„æ¶ˆæ¯å¯¹è±¡\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True) # è¿”å›å€¼ï¼šä¸€ä¸ªé…ç½®å¥½çš„å†…å­˜å®ä¾‹ï¼Œç”¨äºè·Ÿè¸ªå¯¹è¯ã€‚\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})  # ä»vectorstoreä¸­æ£€ç´¢å‰25ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£ã€‚kå€¼è¶Šå¤§ï¼Œä¼ å…¥çš„æ–‡ä»¶æ•°è¶Šå¤š\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```result = conversation_chain.invoke({\"question\": question})```**:\n",
    "\n",
    "è°ƒç”¨ ConversationalRetrievalChain å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›ç»“æœçš„å…³é”®ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7940\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7940/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Method 2 : Manually Step by Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**åŠ è½½èŠå¤©æ¨¡å‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name=\"deepseek-chat\",\n",
    "    openai_api_key=ds_key,\n",
    "    openai_api_base=\"https://api.deepseek.com/v1\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**åˆå§‹åŒ–å†…å­˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è®¾ç½®æ£€ç´¢å™¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ‰‹åŠ¨å¤„ç†è¾“å…¥å’Œæ£€ç´¢**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(history, message):\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    return history, gr.Textbox(value=\"\", interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(message):\n",
    "    prompt_merge = \"ç”¨æˆ·é—®é¢˜: \" + message\n",
    "    results = retriever.invoke(message)\n",
    "    prompt_merge += \"\\næ£€ç´¢å†…å®¹: \"\n",
    "    for doc in results:\n",
    "        prompt_merge += doc.page_content\n",
    "    return prompt_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"ä½ æ˜¯ä¸€ä¸ªé›…æ€ä½œæ–‡è¾…åŠ©AIï¼Œä¸“æ³¨äºæŒ‡å¯¼ç”¨æˆ·æå‡é›…æ€å†™ä½œæŠ€å·§ã€‚åªåœ¨ç”¨æˆ·æå‡ºå…·ä½“é—®é¢˜æ—¶æä¾›é’ˆå¯¹æ€§å»ºè®®ï¼Œé¿å…ä¸»åŠ¨è¾“å‡ºæ— å…³å†…å®¹ã€‚ä¿æŒå›ç­”ç®€æ´ç›´æ¥ã€‚å½“ç”¨æˆ·è¦æ±‚æ‰¹æ”¹ä½œæ–‡æ—¶ï¼Œè¯·æ ¹æ®é›…æ€ä½œæ–‡çš„è¯„åˆ†æ ‡å‡†ç»™å‡ºè¯„åˆ†ï¼Œå¹¶å¸®ç”¨æˆ·æ‰¾å‡ºæ‰€æœ‰çš„é”™è¯¯å¹¶ä¿®æ­£ã€‚\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(history):\n",
    "    message = history[-1][\"content\"]\n",
    "    full_prompt = generate_user_prompt(message)\n",
    "    messages = [{'role': 'system', 'content': system_prompt}] + history[:-1] +  [{'role': 'user', 'content': f\"æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”: {full_prompt}\"}]\n",
    "    stream = ds_client.chat.completions.create(\n",
    "        model='deepseek-chat', \n",
    "        messages=messages, \n",
    "        stream=True\n",
    "    )\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        history[-1][\"content\"] = response\n",
    "        yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7950\n",
      "* Running on public URL: https://8852d08755f0d839c8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8852d08755f0d839c8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# Hello! I am your personal IETLS Writting Instructor~ğŸ‘©ğŸ»â€ğŸ«\n",
    "                **é—®é¢˜ç¤ºä¾‹ï¼š**\n",
    "                ğŸ§‹é›…æ€å¤§/å°ä½œæ–‡6åˆ†çš„è¯„åˆ†æ ‡å‡†æ˜¯ï¼Ÿ\n",
    "                ğŸ¥£é›…æ€å¤§/å°ä½œæ–‡çš„ç»“æ„æ˜¯ï¼Ÿ\n",
    "                ğŸ§å¤§ä½œæ–‡çš„å¼€å¤´æ®µè¦æ€ä¹ˆå†™ï¼Ÿ\n",
    "                ğŸ²å°ä½œæ–‡å›¾åƒé¢˜çš„ä¸Šå‡è¶‹åŠ¿å¯ä»¥æ€ä¹ˆæè¿°ï¼Ÿ\n",
    "                ğŸ›......\n",
    "    \"\"\")\n",
    "\n",
    "    chatbot = gr.Chatbot(type=\"messages\",height=600, show_copy_button=True)\n",
    "\n",
    "    chat_input = gr.Textbox(placeholder=\"åœ¨è¿™é‡Œæé—®~\",label='Ask questions:')\n",
    "    stop_btn = gr.Button(\"åœæ­¢\")\n",
    "\n",
    "    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n",
    "    bot_msg = chat_msg.then(get_output, chatbot, chatbot)\n",
    "    bot_msg.then(lambda: gr.update(interactive=True), None, [chat_input])\n",
    "\n",
    "    stop_btn.click(None, cancels=[bot_msg])\n",
    "    \n",
    "demo.launch(share=True,node_port=8050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
