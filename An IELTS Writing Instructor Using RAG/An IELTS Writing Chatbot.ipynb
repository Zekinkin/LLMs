{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **------------------- Creat an IETLS Writting Instructor with RAG -------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "oai_key = os.getenv('OPENAI_API_KEY')\n",
    "ds_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "oai_client = OpenAI()\n",
    "ds_client = OpenAI(api_key=ds_key, base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **-------------------------------------------------------------------------------------------**\n",
    "# **First of all, we should Create a Chroma KnowledgeBase Base on Local Files**\n",
    "\n",
    "**Step 1 : Load Files**\n",
    "\n",
    "**Step 2 : Split Files into Chunks**\n",
    "\n",
    "**Step 3 : Vector Embedding -- Turn Chunks into Vectors(Using Auto-Encoding LLMS -- OpenAIEmbedding or BERT**\n",
    "\n",
    "**Step 4 : Pass Vectors into DataBase (eg.Chroma,FAISS)**\n",
    "\n",
    "# **-------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1 :  Load Files**\n",
    "\n",
    "**递归遍历文件夹**：需要处理多层嵌套文件夹，使用递归方法获取所有文件路径。\n",
    "\n",
    "**提取文件夹名称作为doc_type**：对于每个文件，记录它所属的直接父文件夹名称。\n",
    "\n",
    "**支持多种文件类型**：根据文件扩展名选择合适的加载器（如PDF、Word、Excel）。\n",
    "\n",
    "**元数据添加**：将父文件夹名添加到每个文件的metadata中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`os`**：定位父文件夹\n",
    "2. **`glob`**：遍历目录获取文件列表\n",
    "3. **`pathlib`**：分析文件属性（如pdf、word、markdown）\n",
    "4. **`LangChain`**：加载文件内容并转为Document对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件夹路径\n",
    "folder = (r\"C:\\Users\\zekin\\projects\\llm_engineering\\My Projects.ipynb\\Week 5 Projects\\IELTS Writting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 支持的文件类型及对应加载器\n",
    "file_loaders = {\n",
    "    \".pdf\": PyPDFLoader,       # PDF处理函数\n",
    "    \".md\": TextLoader,         # Markdown处理函数\n",
    "    \".docx\": Docx2txtLoader,   # Word处理函数\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储所有文档\n",
    "documents = []\n",
    "\n",
    "# 递归获取所有文件并加载\n",
    "def process_folder(folder_path):\n",
    "    for item in glob(os.path.join(folder_path, \"*\")):\n",
    "        if os.path.isdir(item):\n",
    "            # 如果是文件夹，递归处理\n",
    "            process_folder(item)\n",
    "        else:\n",
    "            # 如果是文件，加载并添加元数据\n",
    "            file_ext = Path(item).suffix.lower()\n",
    "            if file_ext in file_loaders:\n",
    "                loader = file_loaders[file_ext](item)  # 讲item传入对应的加载器\n",
    "                docs = loader.load()   # 加载对应的文件\n",
    "                # 获取直接父文件夹名称\n",
    "                doc_type = os.path.basename(os.path.dirname(item))\n",
    "                for doc in docs:\n",
    "                    doc.metadata[\"doc_type\"] = doc_type\n",
    "                    documents.append(doc)\n",
    "            elif file_ext == \".xlsx\":\n",
    "                excel_file = pd.read_excel(item, engine=\"openpyxl\")\n",
    "                content = excel_file.to_string()\n",
    "                doc = Document(page_content=content,metadata={'source':item})\n",
    "                doc_type = os.path.basename(os.path.dirname(item))\n",
    "                doc.metadata['doc_type'] = doc_type\n",
    "                documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2 :  Split Text Into Chunk**\n",
    "\n",
    "**用 langchain 的 CharacterTextSplitter 将文档分割成小块（chunks），便于处理**\n",
    "\n",
    "```chunk_size```: 1000 字符 ≈ 200-300 token  \n",
    "检索任务：较小的块（500-1500）便于精确匹配   \n",
    "总结任务：较大块（1000-2000）保留更多上下文\n",
    "\n",
    "```vchunk_overlap```: 相邻块之间重复的字符数,重叠避免关键信息在块边界丢失  \n",
    "密集信息（如技术文档）：增大重叠（200-300）   \n",
    "稀疏文本（如对话）：减小重叠（50-100）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  # 将大文档按字符数切分为小块，设置重叠以保留上下文\n",
    "chunks = text_splitter.split_documents(documents)  # 将每个文档分割成小块，返回新的文档块列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3 :  Vecot Embedding -- Turn Chunks into Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use **OpenAIEmbeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create our Chroma vectorstore!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 178 documents\n"
     ]
    }
   ],
   "source": [
    "db_name = \"IEKTS_vector_db\"\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get one vector and find how many dimensions it has**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection  # 得到的是 Chroma 内部的 集合对象\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Vectordtore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"vectorstore\")\n",
    "vectorstore.persist()  # 保存到本地文件夹 \"vectorstore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4 :  A Conversation Chain with RAG and Memory + A Gradio Interface**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Method 1 : Use A Pipeline  -- ConversationalRetrievalChain.from_llm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明**\n",
    "- `ConversationBufferMemory` 用于保存对话历史，常与 `ConversationalRetrievalChain` 配合。\n",
    "- `memory_key` 和 `return_messages` 在初始化时设置，影响后续数据访问和格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zekin\\AppData\\Local\\Temp\\ipykernel_39264\\2907641848.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "C:\\Users\\zekin\\AppData\\Local\\Temp\\ipykernel_39264\\2907641848.py:15: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True) # 返回值：一个配置好的内存实例，用于跟踪对话。\n"
     ]
    }
   ],
   "source": [
    "# LangChain 中初始化一个 DeepSeek 聊天模型\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name=\"deepseek-chat\",                  \n",
    "    openai_api_key=ds_key,                         # DeepSeek 的 API 密钥\n",
    "    openai_api_base=\"https://api.deepseek.com/v1\",  # DeepSeek API 端点\n",
    "    streaming=True  # 启用流式输出\n",
    ")\n",
    "\n",
    "# 初始化一个对话缓冲内存\n",
    "# chat_history：后续代码会通过这个键访问历史\n",
    "# return_messages=True: 返回结构化的消息对象\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True) # 返回值：一个配置好的内存实例，用于跟踪对话。\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})  # 从vectorstore中检索前25个最相关的文档。k值越大，传入的文件数越多\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```result = conversation_chain.invoke({\"question\": question})```**:\n",
    "\n",
    "调用 ConversationalRetrievalChain 处理用户问题并返回结果的关键代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7940\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7940/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Method 2 : Manually Step by Step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载聊天模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model_name=\"deepseek-chat\",\n",
    "    openai_api_key=ds_key,\n",
    "    openai_api_base=\"https://api.deepseek.com/v1\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**初始化内存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设置检索器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**手动处理输入和检索**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(history, message):\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    return history, gr.Textbox(value=\"\", interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(message):\n",
    "    prompt_merge = \"用户问题: \" + message\n",
    "    results = retriever.invoke(message)\n",
    "    prompt_merge += \"\\n检索内容: \"\n",
    "    for doc in results:\n",
    "        prompt_merge += doc.page_content\n",
    "    return prompt_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"你是一个雅思作文辅助AI，专注于指导用户提升雅思写作技巧。只在用户提出具体问题时提供针对性建议，避免主动输出无关内容。保持回答简洁直接。当用户要求批改作文时，请根据雅思作文的评分标准给出评分，并帮用户找出所有的错误并修正。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(history):\n",
    "    message = history[-1][\"content\"]\n",
    "    full_prompt = generate_user_prompt(message)\n",
    "    messages = [{'role': 'system', 'content': system_prompt}] + history[:-1] +  [{'role': 'user', 'content': f\"根据以下上下文回答: {full_prompt}\"}]\n",
    "    stream = ds_client.chat.completions.create(\n",
    "        model='deepseek-chat', \n",
    "        messages=messages, \n",
    "        stream=True\n",
    "    )\n",
    "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        history[-1][\"content\"] = response\n",
    "        yield history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7950\n",
      "* Running on public URL: https://8852d08755f0d839c8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8852d08755f0d839c8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# Hello! I am your personal IETLS Writting Instructor~👩🏻‍🏫\n",
    "                **问题示例：**\n",
    "                🧋雅思大/小作文6分的评分标准是？\n",
    "                🥣雅思大/小作文的结构是？\n",
    "                🍧大作文的开头段要怎么写？\n",
    "                🍲小作文图像题的上升趋势可以怎么描述？\n",
    "                🍛......\n",
    "    \"\"\")\n",
    "\n",
    "    chatbot = gr.Chatbot(type=\"messages\",height=600, show_copy_button=True)\n",
    "\n",
    "    chat_input = gr.Textbox(placeholder=\"在这里提问~\",label='Ask questions:')\n",
    "    stop_btn = gr.Button(\"停止\")\n",
    "\n",
    "    chat_msg = chat_input.submit(add_message, [chatbot, chat_input], [chatbot, chat_input])\n",
    "    bot_msg = chat_msg.then(get_output, chatbot, chatbot)\n",
    "    bot_msg.then(lambda: gr.update(interactive=True), None, [chat_input])\n",
    "\n",
    "    stop_btn.click(None, cancels=[bot_msg])\n",
    "    \n",
    "demo.launch(share=True,node_port=8050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
